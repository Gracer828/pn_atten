{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import janome\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import FastText\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_t = Tokenizer()\n",
    "def tokenizer(text): \n",
    "    return [tok for tok in j_t.tokenize(text, wakati=True)]\n",
    "\n",
    "TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True, include_lengths=True, batch_first=True)\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = data.TabularDataset.splits(\n",
    "        path='./', train='train_ja.tsv',\n",
    "        validation='val_ja.tsv', test='test_ja.tsv', format='tsv',\n",
    "        fields=[('Text', TEXT), ('Label', LABEL)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 手順3:バッチ化\n",
    "\n",
    "data.Iterator.splits はdatasetオブジェクトから、各単語を番号に変換してミニバッチごとにまとめた行列を返すイテレータを作成できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = data.Iterator.splits(\n",
    "        (train, val, test), batch_sizes=(2, 2, 1), device=-1, repeat=False,sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "イテレータが返す結果は次のように確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Variable containing:\n",
      " 0  0  3  6  4  2\n",
      " 0  3  7  4  2  1\n",
      "[torch.LongTensor of size 2x6]\n",
      ", \n",
      " 6\n",
      " 5\n",
      "[torch.LongTensor of size 2]\n",
      ")\n",
      "Variable containing:\n",
      " 1\n",
      " 0\n",
      "[torch.LongTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "print(batch.Text)\n",
    "print(batch.Label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch.Textは単語番号の行列と、テキストデータの長さを表す配列のタプルになっています。(data.Fieldの引数でinclude_length=Trueとしたためです)RNNなどの学習で[masked BPTT](https://en.wikipedia.org/wiki/Backpropagation_through_time)を使用する際は、テキストデータのミニバッチと同時に、このテキストの長さのデータを渡します。\n",
    "学習ループでは、このbatch.Textを入力として、Deep Learningモデルによる予測を行い、予測とbatch.Labelを比べて誤差を計算して、学習アルゴリズムを回すことになります。\n",
    "\n",
    "手順2で作成した番号-単語辞書でbatch.Textを変換すると  \n",
    "\n",
    "+ 私 は あなた が unk unk は unk unk unk 。\n",
    "+ 私 は unk が unk です 。 pad pad pad pad\n",
    "\n",
    "となります。padトークンはこのようにバッチ化の際、短いtextに追加して、長さをそろえるために使用されます。デフォルトではミニバッチ内で最も長いtextの長さになるまでpaddingされます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 手順5:attentionつき判別モデルの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの準備が整ったので、文書分類モデルを構築していきます。attentionという機構を組み込んで、予測理由の可視化も行います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. attentionの復習\n",
    "attentionとは（正確な定義ではないですが）予測モデルに入力データのどの部分に注目するか知らせる機構のことです。\n",
    "attention技術は機械翻訳への応用が特に有名です。\n",
    "例えば、日英翻訳モデルを考えます。翻訳モデルは”これはペンです”という文字列を入力として\"This is a pen\"という英文を出力しますが、「pen」という文字を出力する際、モデルは入力文の「ペン」という文字に注目するはずです。このように入力データのある部分に「注目する=attention」という機構を予測モデルに組み込むことで、種々のタスクにおいいて精度が向上することが報告されてきました。\n",
    "また、このattentionを可視化することで「入力データのどの部分に注目して予測を行ったか」という形で予測理由の提示を行うことができます。\n",
    "attentionについての説明と実装は\n",
    "\n",
    "+ [pytorch チュートリアル](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb)\n",
    "\n",
    "がとても参考になります。\n",
    "\n",
    "## 8.2. self attention を利用した分類\n",
    "今回は、attentionの技術を利用して、予測理由が可視化できる文書分類モデルを実装していきます。\n",
    "[self-attentive sentence embedding](https://arxiv.org/pdf/1703.03130.pdf) という論文の手法を単純化したものになります。\n",
    "この手法は次のような手順で予測を行います。\n",
    "\n",
    "1. bidirectional LSTMで文書を変換\n",
    "2. 各単語に対応する隠れ層(下図$h_i$)を入力とし、予測の際その単語に注目すべき確率（self attention 下図$A_i$）をNeural Networkで予測\n",
    "3. self attention の重み付で各単語に対応する隠れ層を足し合わせたものを入力とし、Neural Networkで文書のラベルを予測\n",
    "\n",
    "この$A_i$を可視化してやれば、モデルが予測の際どの単語に注目したかを知ることができます。\n",
    "(オリジナル論文では複数個のself attentionを利用する方法が提案されているのですが、今回は簡易のためattentionは1種類としています。)\n",
    "\n",
    "![image.png](https://qiita-image-store.s3.amazonaws.com/0/183955/8e7d33eb-6182-81c9-7eca-cc138e5f1e02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bidirectional lstmの部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, emb_dim, h_dim, v_size, gpu=True, v_vec=None, batch_first=True):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.gpu = gpu\n",
    "        self.h_dim = h_dim\n",
    "        self.embed = nn.Embedding(v_size, emb_dim)\n",
    "        if v_vec is not None:\n",
    "            self.embed.weight.data.copy_(v_vec)\n",
    "        self.lstm = nn.LSTM(emb_dim, h_dim, batch_first=batch_first,\n",
    "                            bidirectional=True)\n",
    "\n",
    "    def init_hidden(self, b_size):\n",
    "        h0 = Variable(torch.zeros(1*2, b_size, self.h_dim))\n",
    "        c0 = Variable(torch.zeros(1*2, b_size, self.h_dim))\n",
    "        if self.gpu:\n",
    "            h0 = h0.cuda()\n",
    "            c0 = c0.cuda()\n",
    "        return (h0, c0)\n",
    "\n",
    "    def forward(self, sentence, lengths=None):\n",
    "        self.hidden = self.init_hidden(sentence.size(0))\n",
    "        emb = self.embed(sentence)\n",
    "        packed_emb = emb\n",
    "\n",
    "        if lengths is not None:\n",
    "            lengths = lengths.view(-1).tolist()\n",
    "            packed_emb = nn.utils.rnn.pack_padded_sequence(emb, lengths)\n",
    "\n",
    "        out, hidden = self.lstm(packed_emb, self.hidden)\n",
    "\n",
    "        if lengths is not None:\n",
    "            out = nn.utils.rnn.pad_packed_sequence(output)[0]\n",
    "\n",
    "        out = out[:, :, :self.h_dim] + out[:, :, self.h_dim:]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attentionクラスです。\n",
    "LSTMの隠れ層を入力として、各単語へのattentionを出力します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, h_dim):\n",
    "        super(Attn, self).__init__()\n",
    "        self.h_dim = h_dim\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(h_dim, 24),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(24,1)\n",
    "        )\n",
    "\n",
    "    def forward(self, encoder_outputs):\n",
    "        b_size = encoder_outputs.size(0)\n",
    "        attn_ene = self.main(encoder_outputs.view(-1, self.h_dim)) # (b, s, h) -> (b * s, 1)\n",
    "        return F.softmax(attn_ene.view(b_size, -1), dim=1).unsqueeze(2) # (b*s, 1) -> (b, s, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後にattentionを利用して実際に文書分類を行う部分です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnClassifier(nn.Module):\n",
    "    def __init__(self, h_dim, c_num):\n",
    "        super(AttnClassifier, self).__init__()\n",
    "        self.attn = Attn(h_dim)\n",
    "        self.main = nn.Linear(h_dim, c_num)\n",
    "\n",
    "\n",
    "    def forward(self, encoder_outputs):\n",
    "        attns = self.attn(encoder_outputs) #(b, s, 1)\n",
    "        feats = (encoder_outputs * attns).sum(dim=1) # (b, s, h) -> (b, h)\n",
    "        return F.log_softmax(self.main(feats),dim=1), attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習・検証を行う関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epoch, train_iter, optimizer, log_interval=1, batch_size=2):\n",
    "    encoder.train()\n",
    "    classifier.train()\n",
    "    correct = 0\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "        (x, x_l), y = batch.Text, batch.Label\n",
    "        optimizer.zero_grad()\n",
    "        encoder_outputs = encoder(x)\n",
    "        output, attn = classifier(encoder_outputs)\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(y.data.view_as(pred)).cpu().sum()\n",
    "        if idx % log_interval == 0:\n",
    "            print('train epoch: {} [{}/{}], acc:{}, loss:{}'.format(\n",
    "            epoch, (idx+1)*len(x), len(train_iter)*batch_size,\n",
    "            correct/float(log_interval * len(x)),\n",
    "            loss.data[0]))\n",
    "            correct = 0\n",
    "\n",
    "            \n",
    "def test_model(epoch, test_iter):\n",
    "    encoder.eval()\n",
    "    classifier.eval()\n",
    "    correct = 0\n",
    "    for idx, batch in enumerate(test_iter):\n",
    "        (x, x_l), y = batch.Text, batch.Label\n",
    "        encoder_outputs = encoder(x)\n",
    "        output, attn = classifier(encoder_outputs)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(y.data.view_as(pred)).cpu().sum()\n",
    "    print('test epoch:{}, acc:{}'.format(epoch, correct/float(len(test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. 学習実行\n",
    "ハイパーパラメータなどを指定して学習を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN\n",
      "Embedding\n",
      "LSTM\n",
      "AttnClassifier\n",
      "Attn\n",
      "Sequential\n",
      "Linear\n",
      "ReLU\n",
      "Linear\n",
      "Linear\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 300 #単語埋め込み次元\n",
    "h_dim = 32 #lstmの隠れ層の次元\n",
    "class_num = 2 #予測クラウ数\n",
    "lr = 0.001 #学習係数\n",
    "epochs = 3 #エポック数\n",
    "\n",
    " # make model\n",
    "encoder = EncoderRNN(emb_dim, h_dim, len(TEXT.vocab),gpu=False, v_vec = TEXT.vocab.vectors)\n",
    "classifier = AttnClassifier(h_dim, class_num)\n",
    "\n",
    "# init model\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if hasattr(m, 'weight') and (classname.find('Embedding') == -1):\n",
    "        nn.init.xavier_uniform(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "for m in encoder.modules():\n",
    "    print(m.__class__.__name__)\n",
    "    weights_init(m)\n",
    "    \n",
    "for m in classifier.modules():\n",
    "    print(m.__class__.__name__)\n",
    "    weights_init(m)\n",
    "\n",
    "# optim\n",
    "from itertools import chain\n",
    "optimizer = optim.Adam(chain(encoder.parameters(),classifier.parameters()), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 1 [2/4], acc:1.0, loss:0.51763916015625\n",
      "train epoch: 1 [4/4], acc:0.0, loss:1.0013699531555176\n",
      "test epoch:1, acc:0.5\n",
      "train epoch: 2 [2/4], acc:0.5, loss:0.7065788507461548\n",
      "train epoch: 2 [4/4], acc:0.5, loss:0.6915621161460876\n",
      "test epoch:2, acc:0.5\n",
      "train epoch: 3 [2/4], acc:0.5, loss:0.6810123324394226\n",
      "train epoch: 3 [4/4], acc:0.5, loss:0.6845300197601318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/itok/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  \"\"\"\n",
      "/home/itok/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test epoch:3, acc:0.5\n",
      "train epoch: 4 [2/4], acc:0.5, loss:0.5907001495361328\n",
      "train epoch: 4 [4/4], acc:0.5, loss:0.6775313019752502\n",
      "test epoch:4, acc:0.75\n",
      "train epoch: 5 [2/4], acc:0.5, loss:0.6527332663536072\n",
      "train epoch: 5 [4/4], acc:1.0, loss:0.4894612431526184\n",
      "test epoch:5, acc:0.5\n",
      "train epoch: 6 [2/4], acc:0.5, loss:0.9338735342025757\n",
      "train epoch: 6 [4/4], acc:1.0, loss:0.4789973199367523\n",
      "test epoch:6, acc:1.0\n",
      "train epoch: 7 [2/4], acc:1.0, loss:0.6218225359916687\n",
      "train epoch: 7 [4/4], acc:1.0, loss:0.5016450881958008\n",
      "test epoch:7, acc:1.0\n",
      "train epoch: 8 [2/4], acc:1.0, loss:0.6099346876144409\n",
      "train epoch: 8 [4/4], acc:1.0, loss:0.5138865113258362\n",
      "test epoch:8, acc:1.0\n",
      "train epoch: 9 [2/4], acc:1.0, loss:0.5849960446357727\n",
      "train epoch: 9 [4/4], acc:1.0, loss:0.5140206813812256\n",
      "test epoch:9, acc:1.0\n",
      "train epoch: 10 [2/4], acc:1.0, loss:0.6054340600967407\n",
      "train epoch: 10 [4/4], acc:1.0, loss:0.5198944211006165\n",
      "test epoch:10, acc:1.0\n",
      "train epoch: 11 [2/4], acc:1.0, loss:0.6518914699554443\n",
      "train epoch: 11 [4/4], acc:1.0, loss:0.458735853433609\n",
      "test epoch:11, acc:1.0\n",
      "train epoch: 12 [2/4], acc:1.0, loss:0.6148843765258789\n",
      "train epoch: 12 [4/4], acc:1.0, loss:0.459827184677124\n",
      "test epoch:12, acc:1.0\n",
      "train epoch: 13 [2/4], acc:1.0, loss:0.5533528327941895\n",
      "train epoch: 13 [4/4], acc:1.0, loss:0.4504874050617218\n",
      "test epoch:13, acc:1.0\n",
      "train epoch: 14 [2/4], acc:1.0, loss:0.5578677654266357\n",
      "train epoch: 14 [4/4], acc:1.0, loss:0.4602198600769043\n",
      "test epoch:14, acc:1.0\n",
      "train epoch: 15 [2/4], acc:1.0, loss:0.4725040793418884\n",
      "train epoch: 15 [4/4], acc:1.0, loss:0.5186874866485596\n",
      "test epoch:15, acc:1.0\n",
      "train epoch: 16 [2/4], acc:1.0, loss:0.5215688943862915\n",
      "train epoch: 16 [4/4], acc:1.0, loss:0.4593973159790039\n",
      "test epoch:16, acc:1.0\n",
      "train epoch: 17 [2/4], acc:1.0, loss:0.4085765779018402\n",
      "train epoch: 17 [4/4], acc:1.0, loss:0.5228523015975952\n",
      "test epoch:17, acc:1.0\n",
      "train epoch: 18 [2/4], acc:1.0, loss:0.5205334424972534\n",
      "train epoch: 18 [4/4], acc:1.0, loss:0.394381582736969\n",
      "test epoch:18, acc:1.0\n",
      "train epoch: 19 [2/4], acc:1.0, loss:0.4491010904312134\n",
      "train epoch: 19 [4/4], acc:1.0, loss:0.4832098186016083\n",
      "test epoch:19, acc:1.0\n",
      "train epoch: 20 [2/4], acc:1.0, loss:0.46903133392333984\n",
      "train epoch: 20 [4/4], acc:1.0, loss:0.45526981353759766\n",
      "test epoch:20, acc:1.0\n",
      "train epoch: 21 [2/4], acc:1.0, loss:0.45567408204078674\n",
      "train epoch: 21 [4/4], acc:1.0, loss:0.45575574040412903\n",
      "test epoch:21, acc:1.0\n",
      "train epoch: 22 [2/4], acc:1.0, loss:0.5034310817718506\n",
      "train epoch: 22 [4/4], acc:1.0, loss:0.35937973856925964\n",
      "test epoch:22, acc:1.0\n",
      "train epoch: 23 [2/4], acc:1.0, loss:0.45026394724845886\n",
      "train epoch: 23 [4/4], acc:1.0, loss:0.4351778030395508\n",
      "test epoch:23, acc:1.0\n",
      "train epoch: 24 [2/4], acc:1.0, loss:0.34739378094673157\n",
      "train epoch: 24 [4/4], acc:1.0, loss:0.49356889724731445\n",
      "test epoch:24, acc:1.0\n",
      "train epoch: 25 [2/4], acc:1.0, loss:0.44265609979629517\n",
      "train epoch: 25 [4/4], acc:1.0, loss:0.4180002510547638\n",
      "test epoch:25, acc:1.0\n",
      "train epoch: 26 [2/4], acc:1.0, loss:0.4359418749809265\n",
      "train epoch: 26 [4/4], acc:1.0, loss:0.4156334400177002\n",
      "test epoch:26, acc:1.0\n",
      "train epoch: 27 [2/4], acc:1.0, loss:0.4834502637386322\n",
      "train epoch: 27 [4/4], acc:1.0, loss:0.32079970836639404\n",
      "test epoch:27, acc:1.0\n",
      "train epoch: 28 [2/4], acc:1.0, loss:0.47917208075523376\n",
      "train epoch: 28 [4/4], acc:1.0, loss:0.31356775760650635\n",
      "test epoch:28, acc:1.0\n",
      "train epoch: 29 [2/4], acc:1.0, loss:0.4747551679611206\n",
      "train epoch: 29 [4/4], acc:1.0, loss:0.3062426745891571\n",
      "test epoch:29, acc:1.0\n",
      "train epoch: 30 [2/4], acc:1.0, loss:0.4185275435447693\n",
      "train epoch: 30 [4/4], acc:1.0, loss:0.38894546031951904\n",
      "test epoch:30, acc:1.0\n",
      "train epoch: 31 [2/4], acc:1.0, loss:0.4109817147254944\n",
      "train epoch: 31 [4/4], acc:1.0, loss:0.3852693438529968\n",
      "test epoch:31, acc:1.0\n",
      "train epoch: 32 [2/4], acc:1.0, loss:0.38164347410202026\n",
      "train epoch: 32 [4/4], acc:1.0, loss:0.4055750072002411\n",
      "test epoch:32, acc:1.0\n",
      "train epoch: 33 [2/4], acc:1.0, loss:0.37195730209350586\n",
      "train epoch: 33 [4/4], acc:1.0, loss:0.4043695330619812\n",
      "test epoch:33, acc:1.0\n",
      "train epoch: 34 [2/4], acc:1.0, loss:0.3836660385131836\n",
      "train epoch: 34 [4/4], acc:1.0, loss:0.37931352853775024\n",
      "test epoch:34, acc:1.0\n",
      "train epoch: 35 [2/4], acc:1.0, loss:0.3770124316215515\n",
      "train epoch: 35 [4/4], acc:1.0, loss:0.374592661857605\n",
      "test epoch:35, acc:1.0\n",
      "train epoch: 36 [2/4], acc:1.0, loss:0.3717448115348816\n",
      "train epoch: 36 [4/4], acc:1.0, loss:0.36924323439598083\n",
      "test epoch:36, acc:1.0\n",
      "train epoch: 37 [2/4], acc:1.0, loss:0.25772297382354736\n",
      "train epoch: 37 [4/4], acc:1.0, loss:0.4369511604309082\n",
      "test epoch:37, acc:1.0\n",
      "train epoch: 38 [2/4], acc:1.0, loss:0.2515212297439575\n",
      "train epoch: 38 [4/4], acc:1.0, loss:0.4321891665458679\n",
      "test epoch:38, acc:1.0\n",
      "train epoch: 39 [2/4], acc:1.0, loss:0.3425915539264679\n",
      "train epoch: 39 [4/4], acc:1.0, loss:0.37213975191116333\n",
      "test epoch:39, acc:1.0\n",
      "train epoch: 40 [2/4], acc:1.0, loss:0.36950480937957764\n",
      "train epoch: 40 [4/4], acc:1.0, loss:0.33403444290161133\n",
      "test epoch:40, acc:1.0\n",
      "train epoch: 41 [2/4], acc:1.0, loss:0.3458321988582611\n",
      "train epoch: 41 [4/4], acc:1.0, loss:0.3416097164154053\n",
      "test epoch:41, acc:1.0\n",
      "train epoch: 42 [2/4], acc:1.0, loss:0.33906155824661255\n",
      "train epoch: 42 [4/4], acc:1.0, loss:0.3371701240539551\n",
      "test epoch:42, acc:1.0\n",
      "train epoch: 43 [2/4], acc:1.0, loss:0.3347114026546478\n",
      "train epoch: 43 [4/4], acc:1.0, loss:0.3284623324871063\n",
      "test epoch:43, acc:1.0\n",
      "train epoch: 44 [2/4], acc:1.0, loss:0.4025656580924988\n",
      "train epoch: 44 [4/4], acc:1.0, loss:0.20689091086387634\n",
      "test epoch:44, acc:1.0\n",
      "train epoch: 45 [2/4], acc:1.0, loss:0.2725564241409302\n",
      "train epoch: 45 [4/4], acc:1.0, loss:0.4414758086204529\n",
      "test epoch:45, acc:1.0\n",
      "train epoch: 46 [2/4], acc:1.0, loss:0.3882431387901306\n",
      "train epoch: 46 [4/4], acc:1.0, loss:0.19572077691555023\n",
      "test epoch:46, acc:1.0\n",
      "train epoch: 47 [2/4], acc:1.0, loss:0.3087717294692993\n",
      "train epoch: 47 [4/4], acc:1.0, loss:0.3095252811908722\n",
      "test epoch:47, acc:1.0\n",
      "train epoch: 48 [2/4], acc:1.0, loss:0.3885737359523773\n",
      "train epoch: 48 [4/4], acc:1.0, loss:0.19763602316379547\n",
      "test epoch:48, acc:1.0\n",
      "train epoch: 49 [2/4], acc:1.0, loss:0.30194732546806335\n",
      "train epoch: 49 [4/4], acc:1.0, loss:0.30098220705986023\n",
      "test epoch:49, acc:1.0\n",
      "train epoch: 50 [2/4], acc:1.0, loss:0.29783040285110474\n",
      "train epoch: 50 [4/4], acc:1.0, loss:0.29449209570884705\n",
      "test epoch:50, acc:1.0\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "for epoch in range(epochs):\n",
    "    train_model(epoch + 1, train_iter, optimizer)\n",
    "    test_model(epoch + 1, val_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4. attentionの可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spanタグを利用して、attentionを可視化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "from IPython.display import HTML\n",
    "\n",
    "# 単語とattentionの強さを受け取ってspanタグをつける関数\n",
    "def highlight(word, attn):\n",
    "    html_color = u'#%02X%02X%02X' % (255, int(255*(1 - attn)), int(255*(1 - attn)))\n",
    "    return u'<span style=\"background-color: {}\">{}</span>'.format(html_color, word)\n",
    "\n",
    "# 単語番号と対応するattentionの配列を受け取って、spanタグで色付けされた文字列を返す関数\n",
    "def mk_html(sentence, attns):\n",
    "    def itos(word):\n",
    "        word = TEXT.vocab.itos[word]\n",
    "        return word.strip(\"<\").strip(\">\")\n",
    "    html = u\"\"\n",
    "    for word, attn in zip(sentence, attns):\n",
    "        html += u' ' + highlight(\n",
    "        itos(word),\n",
    "        attn\n",
    "        )\n",
    "    return html + u\"<br><br>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/itok/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "正解1:予測1 <span style=\"background-color: #FFC6C6\">unk</span> <span style=\"background-color: #FFC6C6\">unk</span> <span style=\"background-color: #FFDADA\">が</span> <span style=\"background-color: #FFDBDB\">好き</span> <span style=\"background-color: #FFDBDB\">です</span> <span style=\"background-color: #FFDBDB\">。</span><br><br>正解0:予測0 <span style=\"background-color: #FFE5E5\">私</span> <span style=\"background-color: #FFE5E5\">は</span> <span style=\"background-color: #FFDBDB\">unk</span> <span style=\"background-color: #FFE1E1\">が</span> <span style=\"background-color: #FFA8A8\">嫌い</span> <span style=\"background-color: #FFE5E5\">です</span> <span style=\"background-color: #FFE5E5\">。</span><br><br>正解1:予測1 <span style=\"background-color: #FFDCDC\">私</span> <span style=\"background-color: #FFDCDC\">は</span> <span style=\"background-color: #FFCFCF\">unk</span> <span style=\"background-color: #FFDBDB\">が</span> <span style=\"background-color: #FFDCDC\">好き</span> <span style=\"background-color: #FFDCDC\">です</span> <span style=\"background-color: #FFDCDC\">。</span><br><br>正解0:予測0 <span style=\"background-color: #FFD1D1\">unk</span> <span style=\"background-color: #FFD9D9\">が</span> <span style=\"background-color: #FF9292\">嫌い</span> <span style=\"background-color: #FFDFDF\">です</span> <span style=\"background-color: #FFDFDF\">。</span><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "res = \"\"\n",
    "for batch in test_iter:\n",
    "    x = batch.Text[0]\n",
    "    y = batch.Label\n",
    "    encoder_outputs = encoder(x)\n",
    "    output, attn = classifier(encoder_outputs)\n",
    "    pred = output.data.max(1, keepdim=True)[1]\n",
    "    a = attn.data[0,:,0]\n",
    "    res += u\"正解{}:予測{}\".format(str(y[0].data[0]), str(pred[0][0])) + mk_html(x.data[0], a)\n",
    "HTML(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
