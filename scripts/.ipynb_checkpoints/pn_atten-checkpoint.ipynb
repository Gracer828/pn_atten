{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "【 self attention 】簡単に予測理由を可視化できる文書分類モデルを実装する\n",
    "https://qiita.com/itok_msi/items/ad95425b6773985ef959#%E3%82%B3%E3%83%BC%E3%83%89\n",
    "https://github.com/nn116003/self-attention-classification/blob/master/imdb_attn.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "import dill\n",
    "\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec, KeyedVectors\n",
    "\n",
    "# 乾・岡崎研究室の日本語 Wikipedia エンティティベクトル   \n",
    "model = KeyedVectors.load_word2vec_format('../entity_vector/entity_vector.model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1015474, 200)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.vectors\n",
    "\n",
    "print(type(weights))  # numpy.ndarray\n",
    "weights.shape  # (3000000, 300)\n",
    "\n",
    "vocab_size = weights.shape[0]\n",
    "embedding_dim = weights.shape[1]\n",
    "embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# 学習済みの重みをセット\n",
    "embed.weight = nn.Parameter(torch.from_numpy(weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi-LSTMによるエンコーダー\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, emb_dim, h_dim, v_size, gpu=True, v_vec=None, batch_first=True):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.gpu = gpu\n",
    "        self.h_dim = h_dim\n",
    "        self.embed = nn.Embedding(v_size, emb_dim) \n",
    "        if v_vec is not None:\n",
    "            self.embed.weight.data.copy_(v_vec)\n",
    "        self.lstm = nn.LSTM(emb_dim, h_dim, batch_first=batch_first,\n",
    "                            bidirectional=True)\n",
    "\n",
    "    def init_hidden(self, b_size):\n",
    "        h0 = Variable(torch.zeros(1*2, b_size, self.h_dim))\n",
    "        c0 = Variable(torch.zeros(1*2, b_size, self.h_dim))\n",
    "        # GPUがあれば\n",
    "        if self.gpu:\n",
    "            h0 = h0.cuda()\n",
    "            c0 = c0.cuda()\n",
    "        return (h0, c0)\n",
    "\n",
    "    def forward(self, sentence, lengths=None):\n",
    "        self.hidden = self.init_hidden(sentence.size(0))\n",
    "        emb = self.embed(sentence)\n",
    "        packed_emb = emb\n",
    "\n",
    "        if lengths is not None:\n",
    "            lengths = lengths.view(-1).tolist()\n",
    "            packed_emb = nn.utils.rnn.pack_padded_sequence(emb, lengths)\n",
    "\n",
    "        out, hidden = self.lstm(packed_emb, self.hidden)\n",
    "\n",
    "        if lengths is not None:\n",
    "            out = nn.utils.rnn.pad_packed_sequence(output)[0]\n",
    "\n",
    "        out = out[:, :, :self.h_dim] + out[:, :, self.h_dim:]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attentionクラス\n",
    "# LSTMの隠れ層を入力として、各単語へのattentionを出力\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, h_dim):\n",
    "        super(Attn, self).__init__()\n",
    "        self.h_dim = h_dim\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(h_dim, 24),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(24,1)\n",
    "        )\n",
    "\n",
    "    def forward(self, encoder_outputs):\n",
    "        b_size = encoder_outputs.size(0)\n",
    "        attn_ene = self.main(encoder_outputs.view(-1, self.h_dim)) # (b, s, h) -> (b * s, 1)\n",
    "        return F.softmax(attn_ene.view(b_size, -1), dim=1).unsqueeze(2) # (b*s, 1) -> (b, s, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# デコーダー・判別器\n",
    "class AttnClassifier(nn.Module):\n",
    "    def __init__(self, h_dim, c_num):\n",
    "        super(AttnClassifier, self).__init__()\n",
    "        self.attn = Attn(h_dim)\n",
    "        self.main = nn.Linear(h_dim, c_num)\n",
    "\n",
    "\n",
    "    def forward(self, encoder_outputs):\n",
    "        attns = self.attn(encoder_outputs) #(b, s, 1)\n",
    "        feats = (encoder_outputs * attns).sum(dim=1) # (b, s, h) -> (b, h)\n",
    "        return F.log_softmax(self.main(feats)), attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練\n",
    "def train_model(epoch, train_iter, optimizer, log_interval=10):\n",
    "    encoder.train()\n",
    "    classifier.train()\n",
    "    correct = 0\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "        (x, x_l), y = batch.text, batch.label - 1\n",
    "        optimizer.zero_grad()\n",
    "        encoder_outputs = encoder(x)\n",
    "        output, attn = classifier(encoder_outputs)\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(y.data.view_as(pred)).cpu().sum()\n",
    "        if idx % log_interval == 0:\n",
    "            print('train epoch: {} [{}/{}], acc:{}, loss:{}'.format(\n",
    "                epoch, idx*len(x), len(train_iter)*args.batch_size,\n",
    "                correct/float(log_interval * len(x)),\n",
    "                loss.data[0]))\n",
    "            correct = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テスト\n",
    "def test_model(epoch, test_iter):\n",
    "    encoder.eval()\n",
    "    classifier.eval()\n",
    "    correct = 0\n",
    "    for idx, batch in enumerate(test_iter):\n",
    "        (x, x_l), y = batch.text, batch.label - 1\n",
    "        encoder_outputs = encoder(x)\n",
    "        output, attn = classifier(encoder_outputs)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(y.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "    print('test epoch:{}, acc:{}'.format(epoch, correct/float(len(test))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
